{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f33ab6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:09.080805Z",
     "iopub.status.busy": "2025-08-13T07:19:09.079967Z",
     "iopub.status.idle": "2025-08-13T07:19:17.340168Z",
     "shell.execute_reply": "2025-08-13T07:19:17.339304Z"
    },
    "papermill": {
     "duration": 8.265568,
     "end_time": "2025-08-13T07:19:17.341677",
     "exception": false,
     "start_time": "2025-08-13T07:19:09.076109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdffa2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:17.347248Z",
     "iopub.status.busy": "2025-08-13T07:19:17.346904Z",
     "iopub.status.idle": "2025-08-13T07:19:19.110377Z",
     "shell.execute_reply": "2025-08-13T07:19:19.109499Z"
    },
    "papermill": {
     "duration": 1.767743,
     "end_time": "2025-08-13T07:19:19.112000",
     "exception": false,
     "start_time": "2025-08-13T07:19:17.344257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load IMDB dataset from Kaggle input\n",
    "df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "\n",
    "# Encode sentiment: positive → 1, negative → 0\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['review'].values, df['sentiment'].values, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf47d012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:19.117962Z",
     "iopub.status.busy": "2025-08-13T07:19:19.117407Z",
     "iopub.status.idle": "2025-08-13T07:19:29.263043Z",
     "shell.execute_reply": "2025-08-13T07:19:29.262242Z"
    },
    "papermill": {
     "duration": 10.14992,
     "end_time": "2025-08-13T07:19:29.264395",
     "exception": false,
     "start_time": "2025-08-13T07:19:19.114475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20000\n",
      "Padded train shape: torch.Size([40000, 2525])\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using regex\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "# Tokenize\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "# Build vocabulary\n",
    "all_tokens = [token for sublist in train_tokens for token in sublist]\n",
    "counter = Counter(all_tokens)\n",
    "vocab_size = 20000  # top 20,000 words\n",
    "\n",
    "most_common = counter.most_common(vocab_size - 2)\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for idx, (word, _) in enumerate(most_common, start=2):\n",
    "    word2idx[word] = idx\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Encode text\n",
    "def encode(tokens):\n",
    "    return [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "X_train_encoded = [torch.tensor(encode(tokens)) for tokens in train_tokens]\n",
    "X_test_encoded = [torch.tensor(encode(tokens)) for tokens in test_tokens]\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequence(X_train_encoded, batch_first=True, padding_value=0)\n",
    "X_test_padded = pad_sequence(X_test_encoded, batch_first=True, padding_value=0)\n",
    "y_train_tensor = torch.tensor(train_labels)\n",
    "y_test_tensor = torch.tensor(test_labels)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "print(f\"Padded train shape: {X_train_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bd59d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:29.269310Z",
     "iopub.status.busy": "2025-08-13T07:19:29.269096Z",
     "iopub.status.idle": "2025-08-13T07:19:29.275580Z",
     "shell.execute_reply": "2025-08-13T07:19:29.275050Z"
    },
    "papermill": {
     "duration": 0.010083,
     "end_time": "2025-08-13T07:19:29.276588",
     "exception": false,
     "start_time": "2025-08-13T07:19:29.266505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = IMDBDataset(X_train_padded, y_train_tensor)\n",
    "test_dataset = IMDBDataset(X_test_padded, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf88240a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:29.281887Z",
     "iopub.status.busy": "2025-08-13T07:19:29.281261Z",
     "iopub.status.idle": "2025-08-13T07:19:40.641273Z",
     "shell.execute_reply": "2025-08-13T07:19:40.640642Z"
    },
    "papermill": {
     "duration": 11.363921,
     "end_time": "2025-08-13T07:19:40.642600",
     "exception": false,
     "start_time": "2025-08-13T07:19:29.278679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, word2idx, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    matrix_len = len(word2idx)\n",
    "    weights_matrix = np.zeros((matrix_len, embedding_dim))\n",
    "\n",
    "    for word, i in word2idx.items():\n",
    "        weights_matrix[i] = embeddings_index.get(word, np.random.normal(scale=0.6, size=(embedding_dim,)))\n",
    "    \n",
    "    return torch.tensor(weights_matrix, dtype=torch.float32)\n",
    "\n",
    "# Use GloVe from Kaggle input\n",
    "glove_path = \"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n",
    "embedding_dim = 100\n",
    "glove_weights = load_glove_embeddings(glove_path, word2idx, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946aab7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:40.650384Z",
     "iopub.status.busy": "2025-08-13T07:19:40.650128Z",
     "iopub.status.idle": "2025-08-13T07:19:40.659284Z",
     "shell.execute_reply": "2025-08-13T07:19:40.658647Z"
    },
    "papermill": {
     "duration": 0.015529,
     "end_time": "2025-08-13T07:19:40.660376",
     "exception": false,
     "start_time": "2025-08-13T07:19:40.644847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "        self.rnn = nn.RNN(embedding_weights.shape[1], 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, h_n = self.rnn(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n.squeeze(0)))\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_weights.shape[1], 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n[-1]))\n",
    "\n",
    "class RNNLearned(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, h_n = self.rnn(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n.squeeze(0)))\n",
    "\n",
    "class LSTMLearned(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(embedded)\n",
    "        return torch.sigmoid(self.fc(h_n[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13de5cf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:40.664981Z",
     "iopub.status.busy": "2025-08-13T07:19:40.664735Z",
     "iopub.status.idle": "2025-08-13T07:19:40.671030Z",
     "shell.execute_reply": "2025-08-13T07:19:40.670442Z"
    },
    "papermill": {
     "duration": 0.009681,
     "end_time": "2025-08-13T07:19:40.672048",
     "exception": false,
     "start_time": "2025-08-13T07:19:40.662367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch).squeeze()\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    evaluate_model(model, val_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            output = model(x_batch).squeeze().cpu().numpy() > 0.5\n",
    "            preds.extend(output)\n",
    "            labels.extend(y_batch.numpy())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ca619e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T07:19:40.677105Z",
     "iopub.status.busy": "2025-08-13T07:19:40.676549Z",
     "iopub.status.idle": "2025-08-13T07:29:44.367400Z",
     "shell.execute_reply": "2025-08-13T07:29:44.366337Z"
    },
    "papermill": {
     "duration": 603.694552,
     "end_time": "2025-08-13T07:29:44.368655",
     "exception": false,
     "start_time": "2025-08-13T07:19:40.674103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 RNN + GloVe\n",
      "Epoch 1, Loss: 0.6976\n",
      "Epoch 2, Loss: 0.6945\n",
      "Epoch 3, Loss: 0.6945\n",
      "Epoch 4, Loss: 0.6957\n",
      "Epoch 5, Loss: 0.6951\n",
      "Accuracy: 0.5040\n",
      "\n",
      "🔹 LSTM + GloVe\n",
      "Epoch 1, Loss: 0.6937\n",
      "Epoch 2, Loss: 0.6932\n",
      "Epoch 3, Loss: 0.6932\n",
      "Epoch 4, Loss: 0.6932\n",
      "Epoch 5, Loss: 0.6932\n",
      "Accuracy: 0.5039\n",
      "\n",
      "🔹 RNN + Learned Embedding\n",
      "Epoch 1, Loss: 0.6956\n",
      "Epoch 2, Loss: 0.6946\n",
      "Epoch 3, Loss: 0.6939\n",
      "Epoch 4, Loss: 0.6939\n",
      "Epoch 5, Loss: 0.6943\n",
      "Accuracy: 0.4960\n",
      "\n",
      "🔹 LSTM + Learned Embedding\n",
      "Epoch 1, Loss: 0.6936\n",
      "Epoch 2, Loss: 0.6932\n",
      "Epoch 3, Loss: 0.6932\n",
      "Epoch 4, Loss: 0.6932\n",
      "Epoch 5, Loss: 0.6932\n",
      "Accuracy: 0.4960\n"
     ]
    }
   ],
   "source": [
    "print(\"🔹 RNN + GloVe\")\n",
    "model_rnn_glove = RNNModel(glove_weights)\n",
    "train_model(model_rnn_glove, train_loader, test_loader)\n",
    "\n",
    "print(\"\\n🔹 LSTM + GloVe\")\n",
    "model_lstm_glove = LSTMModel(glove_weights)\n",
    "train_model(model_lstm_glove, train_loader, test_loader)\n",
    "\n",
    "print(\"\\n🔹 RNN + Learned Embedding\")\n",
    "model_rnn_learned = RNNLearned(len(word2idx))\n",
    "train_model(model_rnn_learned, train_loader, test_loader)\n",
    "\n",
    "print(\"\\n🔹 LSTM + Learned Embedding\")\n",
    "model_lstm_learned = LSTMLearned(len(word2idx))\n",
    "train_model(model_lstm_learned, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 642.664086,
   "end_time": "2025-08-13T07:29:47.089733",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-13T07:19:04.425647",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
